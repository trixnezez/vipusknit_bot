{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa58e0-55f3-4870-9278-8785330b24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import xmltojson\n",
    "import utils\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "global_url = \"https://krisha.kz\"\n",
    "\n",
    "# headers = {\n",
    "#     \"user-agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\",\n",
    "#     \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"\n",
    "# }\n",
    "\n",
    "proxy_list = [\n",
    "        {'https': f'http://vj6eE0:p2XaGJ@212.81.39.27:9932'},\n",
    "        {'https': f'http://vj6eE0:p2XaGJ@212.81.38.13:9228'},\n",
    "        {'https': f'http://vj6eE0:p2XaGJ@212.81.36.134:9834'},\n",
    "        {'https': f'http://vj6eE0:p2XaGJ@212.81.37.136:9924'},\n",
    "        {'https': f'http://vj6eE0:p2XaGJ@212.81.38.237:9887'}\n",
    "    ]\n",
    "\n",
    "headers = [{'headers': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36'},\n",
    "    {'headers': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.72 Safari/537.36'},\n",
    "    {'headers': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10) AppleWebKit/600.1.25 (KHTML, like Gecko) Version/8.0 Safari/600.1.25'},\n",
    "    {'headers': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0'},\n",
    "    {'headers': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36'},\n",
    "    {'headers': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36'},\n",
    "    {'headers': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.1.17 (KHTML, like Gecko) Version/7.1 Safari/537.85.10'},\n",
    "    {'headers': 'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko'},\n",
    "    {'headers': 'Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0'},\n",
    "    {'headers': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.104 Safari/537.36'}\n",
    "]\n",
    "\n",
    "date = datetime.now().strftime(\"%d/%m/%Y %H:%M:S\")\n",
    " \n",
    "    \n",
    "def get_all_pages():\n",
    "    proxies = random.choice(proxy_list)\n",
    "    \n",
    "    req = requests.get(url = \"https://krisha.kz/arenda/kvartiry/almaty/?rent-period-switch=%2Farenda%2Fkvartiry&page=2\", headers = random.choice(headers), proxies = proxies)\n",
    "    with open(\"data/page_1.html\",\"w\",encoding =\"utf-8\") as file:\n",
    "            file.write(req.text)\n",
    "\n",
    "    with open(\"data/page_1.html\",encoding =\"utf-8\") as file:\n",
    "        src = file.read()\n",
    "    soup = BeautifulSoup(src,\"lxml\")\n",
    "    #find(\"span\", {\"class\":\"pager-item-not-in-short-range\"}).\n",
    "    pages_count = int(soup.find(\"nav\",{\"class\":\"paginator\"}).find_all(\"a\")[-2].text)\n",
    "    for i in range(1,1750):\n",
    "        url = f\"https://krisha.kz/arenda/kvartiry/almaty/?rent-period-switch=%2Farenda%2Fkvartiry&page={i}\"\n",
    "        r = requests.get(url = url,headers = random.choice(headers),proxies = random.choice(proxy_list))\n",
    "        print(f'Вытаскиваю страницы по этому прокси: {proxies}')\n",
    "        with open(f\"data/page_{i}.html\",\"w\",encoding =\"utf-8\") as file:\n",
    "            file.write(r.text)\n",
    "\n",
    "    return 1750\n",
    "\n",
    "\n",
    "# Прокси гамно, долгий response =\\\n",
    "# def get_proxy():\n",
    "#     url = \"https://free-proxy-list.net/\"\n",
    "#     # формируем объект sp, получив ответ http\n",
    "#     sp = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "#     proxy = []\n",
    "#     for row in sp.find(\"div\", attrs={\"class\": \"fpl-list\"}).find_all(\"tr\")[1:]:\n",
    "#         tds = row.find_all(\"td\")\n",
    "#         try:\n",
    "#             ip = tds[0].text.strip()\n",
    "#             port = tds[1].text.strip()\n",
    "#             host = f\"{ip}:{port}\"\n",
    "#             proxy.append(host)\n",
    "#         except IndexError:\n",
    "#             continue\n",
    "#     return proxy\n",
    "\n",
    "def collect_data(pages_count):\n",
    "    for page in range(1, pages_count+1):\n",
    "        print(f'Обрабатывается страница № {page}')\n",
    "        with open(f\"data/page_{page}.html\",encoding =\"utf-8\") as file:\n",
    "            src = file.read()\n",
    "            soup = BeautifulSoup(src,\"lxml\")\n",
    "            items_divs = soup.find_all(\"div\",{\"class\":\"a-card\"})\n",
    "            #print(len(items_divs))\n",
    "            urls =[]\n",
    "            for item in items_divs:\n",
    "                item_url = item.find(\"div\",{\"class\":\"a-card__header\"}).find(\"a\",{\"class\":\"a-card__title\"}).get(\"href\")\n",
    "                urls.append(item_url)\n",
    "            with open(\"items_urls.txt\",\"w\",encoding =\"utf-8\") as file:\n",
    "                for url in urls:\n",
    "                    file.write(f\"{global_url}{url}\\n\")\n",
    "            get_data(file_path=\"items_urls.txt\")\n",
    "            #print(urls)\n",
    "\n",
    "\n",
    "def get_data(file_path):\n",
    "    result_list = []\n",
    "    with open(file_path,encoding =\"utf-8\") as file:\n",
    "        urls_list = file.readlines()\n",
    "        clear_urls_list =[]\n",
    "        for url in urls_list:\n",
    "            url = url.strip()\n",
    "            clear_urls_list.append(url)\n",
    "    \n",
    "\n",
    "    for url in clear_urls_list:\n",
    "\n",
    "        proxies = random.choice(proxy_list)\n",
    "        header = random.choice(headers)\n",
    "\n",
    "        print(f'header: {header}\\n proxy: {proxies}')\n",
    "        #подгружаю html \n",
    "        response = requests.get(url=url,headers=header,proxies = proxies)\n",
    "        soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "        try:\n",
    "            \n",
    "            if soup.find(\"header\").find(\"h1\"):\n",
    "                if soup.find(\"header\").find(\"h1\").text == \"Никого нет дома\":\n",
    "                    print(f'{url} not valid url')\n",
    "                    return\n",
    "            else:\n",
    "                pass\n",
    "        except exception as e:\n",
    "            return \n",
    "        #подгружаю json\n",
    "        script = soup.find(\"script\",{\"id\":\"jsdata\"}).text[20:]\n",
    "        script = script[:-6]\n",
    "        #print(script)\n",
    "        json_data = json.loads(script)\n",
    "        price = json_data.get('advert').get('price')\n",
    "        title = json_data.get('advert').get('title')\n",
    "        square = json_data.get('advert').get('square')\n",
    "        rooms = json_data.get('advert').get('rooms')\n",
    "        x = json_data.get('advert').get('map').get('lon')\n",
    "        y = json_data.get('advert').get('map').get('lat')\n",
    "        address = json_data.get('advert').get('addressTitle')\n",
    "        district = json_data.get('advert').get('address').get('district')\n",
    "        category = json_data['adverts'][0].get('category').get('label')\n",
    "        owner = json_data['adverts'][0].get('owner').get('title')\n",
    "        owner_category = json_data['adverts'][0].get('owner').get('label').get('title')\n",
    "        date = json_data['adverts'][0].get('addedAt')\n",
    "\n",
    "        #print(price)\n",
    "        #print(price,title,square,rooms,x,y,address,district,category,owner,owner_category,date)\n",
    "        # for i in range(0,len(vacancies)):\n",
    "        #     urls.append(vacancies[i]['links']['desktop'])\n",
    "\n",
    "        rent_category = 'Помесячно'\n",
    "        with open(f'rent_by_month_{date}.csv','a',encoding =\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(\n",
    "                (\n",
    "                    title,\n",
    "                    price,\n",
    "                    square,\n",
    "                    rooms,\n",
    "                    x,\n",
    "                    y,\n",
    "                    address,\n",
    "                    district,\n",
    "                    category,\n",
    "                    owner,\n",
    "                    owner_category,\n",
    "                    date,\n",
    "                    rent_category\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    with open(f'rent_by_month_{date}.csv','w',encoding =\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(\n",
    "                (\n",
    "                    'Описание',\n",
    "                    \"Цена\",\n",
    "                    \"Площадь\",\n",
    "                    \"Количество комнат\",\n",
    "                    \"x\",\n",
    "                    \"y\",\n",
    "                    \"Адрес\",\n",
    "                    \"Район\",\n",
    "                    \"Категория на сайте\",\n",
    "                    \"Владелец\",\n",
    "                    \"Категория владельца\",\n",
    "                    \"Дата обьявления\",\n",
    "                    \"Срок аренды\"\n",
    "                )\n",
    "            )\n",
    "    pages_count = get_all_pages()\n",
    "\n",
    "    collect_data(pages_count=pages_count)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
